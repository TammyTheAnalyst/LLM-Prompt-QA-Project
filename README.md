# LLM Prompt QA Project

This project simulates how AI teams evaluate GPT-4 responses for accuracy, tone, clarity, and helpfulness.

It reflects real-world work I do at Telus, where I review and score AI outputs across text, voice, and visual content.

---

## ğŸ“Œ Project Goal

I evaluated outputs from OpenAIâ€™s GPT-4 using LangChain and Python to assess tone, clarity, and accuracy.


---

## ğŸ› ï¸ Tools Used

- Python
- LangChain
- Google Colab
- GitHub



---

## ğŸ¯ Relevant Job Titles

This kind of work is used in:
- LLM Evaluation Specialist
- AI Content QA Analyst
- Prompt QA Reviewer
- NLP Data Specialist
- Human-in-the-Loop (HITL) QA Assistant

---

## ğŸ“‚ Project Structure

- `notebook/`: where the code and LangChain test lives  
- `images/`: screenshots for documentation and GitHub visuals  
- `data/`: (optional) saved scores and evaluations

---

## âœ… Status Checklist

- [x] Create GitHub repo  
- [x] Add folders  
- [x] Write first prompt  
- [x] Run it using LangChain  
- [x] Score the output  
- [x] Take screenshots  
- [ ] Publish on LinkedIn

---

ğŸš€ Why This Project Matters to Employers (for Recruiters)

âœ… I evaluated GPT-4 outputs using LangChain and Python â€” simulating real-world prompt QA workflows

âœ… I built a scoring system to assess tone, clarity, and factual accuracy, based on methods I used when grading AI outputs at Telus

âœ… I worked end-to-end: writing prompts, running model responses, and structuring results for review

âœ… This project demonstrates my hands-on ability to review LLM responses using modern tools and methods

