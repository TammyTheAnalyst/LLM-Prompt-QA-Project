# LLM Prompt QA Project

This project simulates how AI teams evaluate GPT-4 responses for accuracy, tone, clarity, and helpfulness.

It reflects real-world work I do at Telus, where I review and score AI outputs across text, voice, and visual content.

---

## ğŸ“Œ Project Goal

To evaluate responses from GPT using LangChain and Python, and build a lightweight scoring system to judge:
- Clarity
- Factual accuracy
- Tone and relevance

---

## ğŸ› ï¸ Tools Used

- Python
- LangChain
- Google Colab
- GitHub

---

## ğŸ“‚ Project Structure

- `notebook/`: where the code and LangChain test lives  
- `images/`: screenshots for documentation and GitHub visuals  
- `data/`: (optional) saved scores and evaluations

---

## ğŸ¯ Relevant Job Titles

This kind of work is used in:
- LLM Evaluation Specialist
- AI Content QA Analyst
- Prompt QA Reviewer
- NLP Data Specialist
- Human-in-the-Loop (HITL) QA Assistant

---

## âœ… Status Checklist

- [x] Create GitHub repo  
- [x] Add folders  
- [ ] Write first prompt  
- [ ] Run it using LangChain  
- [ ] Score the output  
- [ ] Take screenshots  
- [ ] Publish on LinkedIn  
