# LLM Prompt QA Project

This project simulates how AI teams evaluate GPT-4 responses for accuracy, tone, clarity, and helpfulness.

It reflects real-world work I do at Telus, where I review and score AI outputs across text, voice, and visual content.

---

## 📌 Project Goal

I evaluated outputs from OpenAI’s GPT-4 using LangChain and Python to assess tone, clarity, and accuracy.


---

## 🛠️ Tools Used

- Python
- LangChain
- Google Colab
- GitHub



---

## 🎯 Relevant Job Titles

This kind of work is used in:
- LLM Evaluation Specialist
- AI Content QA Analyst
- Prompt QA Reviewer
- NLP Data Specialist
- Human-in-the-Loop (HITL) QA Assistant

---

## 📂 Project Structure

- `notebook/`: where the code and LangChain test lives  
- `images/`: screenshots for documentation and GitHub visuals  
- `data/`: (optional) saved scores and evaluations

---

## ✅ Status Checklist

- [x] Create GitHub repo  
- [x] Add folders  
- [x] Write first prompt  
- [x] Run it using LangChain  
- [x] Score the output  
- [x] Take screenshots  
- [ ] Publish on LinkedIn

---

🚀 Why This Project Matters to Employers (for Recruiters)

✅ I evaluated GPT-4 outputs using LangChain and Python — simulating real-world prompt QA workflows

✅ I built a scoring system to assess tone, clarity, and factual accuracy, based on methods I used when grading AI outputs at Telus

✅ I worked end-to-end: writing prompts, running model responses, and structuring results for review

✅ This project demonstrates my hands-on ability to review LLM responses using modern tools and methods

